{
  "project_id": "test-apisix-routing",
  "project_name": "Test APISIX + Inference Combined",
  "version": "1.0.0",
  "description": "Test project combining APISIX gateway with inference endpoints",
  "owner": "test-team",
  "team": [
    "test@example.com"
  ],
  "tags": [
    "test",
    "apisix",
    "inference",
    "combined"
  ],
  "environment": "test",
  "created_at": "2025-09-27 21:51:39.521806",
  "updated_at": "2025-09-27 21:51:39.522229",
  "modules": [
    {
      "module_type": "inference_endpoint",
      "name": "test-llm-service",
      "version": "1.0.0",
      "status": "enabled",
      "description": "Test LLM Inference Service",
      "dependencies": [],
      "cross_references": {
        "gateway": {
          "module_name": "test-apisix-gateway",
          "module_type": "api_gateway",
          "purpose": "Receive routed requests from APISIX gateway",
          "required": true,
          "fallback": null
        }
      },
      "environment_overrides": {},
      "config": {
        "model_name": "llama-3.1-8b-instant",
        "model_version": "latest",
        "endpoint_url": "http://localhost:9080/v1/chat/completions",
        "system_prompt": "You are a helpful AI assistant.",
        "max_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.9,
        "batch_size": 1
      }
    },
    {
      "module_type": "api_gateway",
      "name": "test-apisix-gateway",
      "version": "1.0.0",
      "status": "enabled",
      "description": "Test APISIX Gateway with LLM routing",
      "dependencies": [
        "test-llm-service"
      ],
      "cross_references": {
        "llm_service": {
          "module_name": "test-llm-service",
          "module_type": "inference_endpoint",
          "purpose": "Route inference requests to LLM service",
          "required": true,
          "fallback": null
        }
      },
      "environment_overrides": {},
      "config": {
        "admin_api_url": "http://localhost:9180",
        "admin_key": "edd1c9f034335f136f87ad84b625c8f1",
        "gateway_url": "http://localhost:9080",
        "dashboard_url": "http://localhost:9000",
        "routes": [
          {
            "name": "llm-inference-route",
            "uri": "/v1/inference/*",
            "methods": [
              "POST",
              "GET",
              "OPTIONS"
            ],
            "upstream_id": "test-apisix-routing-groq-upstream",
            "service_id": null,
            "plugins": [
              {
                "name": "limit-req",
                "enabled": true,
                "config": {
                  "rate": 10,
                  "burst": 5,
                  "rejected_code": 429,
                  "key_type": "var",
                  "key": "remote_addr",
                  "rejected_msg": "Rate limit exceeded for inference requests"
                },
                "priority": null
              },
              {
                "name": "ai-prompt-template",
                "enabled": true,
                "config": {
                  "templates": [
                    {
                      "name": "groq-llama-template",
                      "template": {
                        "model": "llama-3.1-8b-instant",
                        "messages": [
                          {
                            "role": "system",
                            "content": "You are a helpful AI assistant. Respond concisely and accurately."
                          },
                          {
                            "role": "user",
                            "content": "{{prompt}}"
                          }
                        ]
                      }
                    }
                  ]
                },
                "priority": null
              },
              {
                "name": "proxy-rewrite",
                "enabled": true,
                "config": {
                  "regex_uri": [
                    "^/v1/inference/(.*)",
                    "/v1/chat/completions"
                  ],
                  "headers": {
                    "Authorization": "Bearer <API_KEY>",
                    "Content-Type": "application/json",
                    "Accept-Encoding": "identity",
                    "X-Gateway-Service": "test-apisix-gateway",
                    "X-Target-Service": "groq-llm-service"
                  }
                },
                "priority": null
              }
            ],
            "host": null,
            "priority": 0,
            "vars": null
          },
          {
            "name": "llm-chat-route",
            "uri": "/v1/chat/completions",
            "methods": [
              "POST",
              "OPTIONS"
            ],
            "upstream_id": "test-apisix-routing-groq-upstream",
            "service_id": null,
            "plugins": [
              {
                "name": "limit-req",
                "enabled": true,
                "config": {
                  "rate": 20,
                  "burst": 10,
                  "rejected_code": 429,
                  "key_type": "var",
                  "key": "remote_addr",
                  "rejected_msg": "Rate limit exceeded for chat requests"
                },
                "priority": null
              },
              {
                "name": "proxy-rewrite",
                "enabled": true,
                "config": {
                  "headers": {
                    "Authorization": "Bearer <API_KEY>",
                    "Content-Type": "application/json",
                    "Accept-Encoding": "identity",
                    "X-Gateway-Service": "test-apisix-gateway",
                    "X-Target-Service": "groq-llm-service"
                  }
                },
                "priority": null
              }
            ],
            "host": null,
            "priority": 0,
            "vars": null
          },
          {
            "name": "test-route",
            "uri": "/test",
            "methods": [
              "GET"
            ],
            "upstream_id": null,
            "service_id": null,
            "plugins": [
              {
                "name": "serverless-pre-function",
                "enabled": true,
                "config": {
                  "phase": "access",
                  "functions": [
                    "return function(conf, ctx) ngx.say('{\"message\":\"Hello from combined APISIX + Inference test\",\"timestamp\":\"' .. os.date() .. '\",\"services\":[\"test-apisix-gateway\",\"test-llm-service\"]}') ngx.exit(200) end"
                  ]
                },
                "priority": null
              }
            ],
            "host": null,
            "priority": 0,
            "vars": null
          }
        ],
        "upstreams": [
          {
            "name": "groq-upstream",
            "type": "roundrobin",
            "nodes": {
              "api.groq.com:443": 100
            },
            "timeout": {
              "connect": 10,
              "send": 30,
              "read": 60
            },
            "retries": 2,
            "health_check": null
          }
        ],
        "global_plugins": [
          {
            "name": "cors",
            "enabled": true,
            "config": {
              "allow_origins": "*",
              "allow_methods": "GET, POST, PUT, DELETE, OPTIONS",
              "allow_headers": "*",
              "max_age": 3600
            },
            "priority": null
          }
        ],
        "jwt_auth_enabled": false,
        "rate_limiting_enabled": true,
        "logging_enabled": true,
        "prometheus_enabled": false,
        "ssl_enabled": false,
        "ssl_cert": null,
        "ssl_key": null,
        "cors_enabled": true,
        "cors_origins": [
          "*"
        ],
        "cors_methods": [
          "GET",
          "POST",
          "PUT",
          "DELETE",
          "OPTIONS"
        ],
        "default_timeout": 60,
        "default_retries": 2,
        "streaming_enabled": true,
        "response_buffering": false,
        "request_buffering": true
      }
    }
  ],
  "metadata": {}
}